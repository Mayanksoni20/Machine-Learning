
# coding: utf-8

# In[1]:

#Loading all the libraries
import pyPdf
import os
import pandas as pd
import string
import numpy as np
from nltk.corpus import stopwords
import xgboost as xgb


# In[2]:

#Setting the path for approved dataset
my_path = '' #This path is Users input , obtained by clicking 'Browse' and selecting file from local machine.
my_file = '' # User input file

#Setting the path for Conditionally approved dataset
my_path2 = '' #This path is Users input , obtained by clicking 'Browse' and selecting file from local machine.
my_file2 = '' # User input file

#Setting the path for Rejected Dataset
my_path3 = '' #This path is Users input , obtained by clicking 'Browse' and selecting file from local machine.
my_file3 = '' # User input file


# In[3]:

#Function to load pdf files , i am sure there is a better way to do this.
def split(my_path,my_file):
    pdf = pyPdf.PdfFileReader(open(my_path+my_file, "rb"))
    all_text3 =''
    for page in pdf.pages:
        all_text3 = all_text3 + str(page.extractText().encode('utf-8'))

    return all_text3

def split2(my_path2,my_file2):
    pdf = pyPdf.PdfFileReader(open(my_path2+my_file2, "rb"))
    all_text3 =''
    for page in pdf.pages:
        all_text3 = all_text3 + str(page.extractText().encode('utf-8'))

    return all_text3

def split3(my_path3,my_file3):
    pdf = pyPdf.PdfFileReader(open(my_path3+my_file3, "rb"))
    all_text3 =''
    for page in pdf.pages:
        all_text3 = all_text3 + str(page.extractText().encode('utf-8'))

    return all_text3


# In[4]:

#Loading data in to a list
all_files = os.listdir(my_path)
approved_doc = []
all_files = all_files[1:]
for files in all_files:
    try:
        singledoc = split(my_path,files)
        approved_doc.append(singledoc)
    except Exception:
        pass


# In[5]:

#Loading data in a list
all_files2 = os.listdir(my_path2)
approved_doc2 = []
all_files2 = all_files2[1:]
for files in all_files2:
    try:
        singledoc = split(my_path2,files)
        approved_doc2.append(singledoc)
    except Exception:
        pass


# In[6]:

#Loading data in a list
all_files3 = os.listdir(my_path3)
approved_doc3 = []
all_files3 = all_files3[1:]
for files in all_files3:
    try:
        singledoc = split(my_path3,files)
        approved_doc3.append(singledoc)
    except Exception:
        pass


# In[7]:

# Assigning Numbers to Categorisations
# 0 = ""
# 1 = ""
# 2 = ""
# Assigning dataframes to categories
# df = ""
# df1 = ""
# df2 = ""


# In[8]:

df = pd.DataFrame()
#df['column1'] = all_files.index
df ['column2'] = ""
df['column3'] = 1


# In[9]:

df1 = pd.DataFrame()
df1['column1'] = ""
df1['column2'] = 2


# In[10]:

df2 = pd.DataFrame()
df2['column1'] = approved_doc3
df2 ['column2'] = 0


# In[11]:

# appending df and df1 to df3
df3 = df1.append(df)
df3.index = range(0,df3.shape[0]) 


# In[12]:

# appending df2,created previously, to df4 , Final Dataframe with training dataset is in df4.
df4 = df3.append(df2)
df4.index = range(0,df4.shape[0])


# In[13]:

#cleaning the text by removing punctuations and turning everything lower.Not removing the stopwords because of the words
# - like "should" and "must"
#stops = set(stopwords.words("english"))
import re
clean_rows = []
for rows in df4.column1:
    rows = rows.lower()
    rows = re.sub('[^A-Za-z]+', ' ', rows)
    #rows = ' '.join([word for word in rows.split() if word not in stops])
    #rows=rows.strip()
    clean_rows.append(rows)


# In[14]:

#lets assign the new list to the dataframe
df4['column1'] = clean_rows


# In[15]:

#creating tfidf 
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(analyzer = "word",                                tokenizer = None,                                 preprocessor = None,                              stop_words = None,                                max_features = 1000) 

train_test_tdidf_features = vectorizer.fit_transform(df4.content)
train_test_tdidf_features = train_test_tdidf_features.toarray()

df_train_test_tfidf_features = pd.DataFrame(train_test_tdidf_features)
#df_train_test_tfidf_features.to_csv('dummy.csv')


# In[16]:

df_train_test_tfidf_features.columns = vectorizer.get_feature_names()


# In[17]:

#df_train_test_tfidf_features.head(3)


# In[18]:

#df_train_test_tfidf_features.shape


# In[19]:

#Splitting the data into train and test sets using SVD feature
y_train = df4.categorisation
from sklearn.cross_validation import train_test_split

x_train, x_valid, y_train, y_valid = train_test_split(df_train_test_tfidf_features, y_train, test_size=0.15, random_state=4242)


# In[20]:

#Checking if all the datasets look good. Looks good
#x_train
#x_valid
#y_train
#y_valid
#df4_validation
#df4_train


# In[21]:

#XGB Softmax
d_train = xgb.DMatrix(x_train, label=y_train)
d_valid = xgb.DMatrix(x_valid, label=y_valid)

params = {}
#params['objective'] = 'binary:logistic'
#params['objective'] = 'multi:softmax'
params['objective'] = 'multi:softprob'
params['eval_metric'] = 'merror'
params['eta'] = 0.3
params['max_depth'] = 10
params["subsample"] = 0.7
params['num_class'] = 3 # Three Classes
params["min_child_weight"] = 1
params["colsample_bytree"] = 0.65

watchlist = [(d_train, 'train'), (d_valid, 'valid')]

model = xgb.train(params, d_train, 200, watchlist, early_stopping_rounds=20)


# In[22]:

d_test = xgb.DMatrix(x_valid)
prediction = pd.DataFrame(model.predict(d_test,ntree_limit=3))


# In[23]:

#XGB Softmax
d_train = xgb.DMatrix(x_train, label=y_train)
d_valid = xgb.DMatrix(x_valid, label=y_valid)

params = {}
#params['objective'] = 'binary:logistic'
params['objective'] = 'multi:softmax'
#params['objective'] = 'multi:softprob'
params['eval_metric'] = 'merror'
params['eta'] = 0.3
params['max_depth'] = 10
params["subsample"] = 0.7
params['num_class'] = 3 # Three Classes
params["min_child_weight"] = 1
params["colsample_bytree"] = 0.65

watchlist = [(d_train, 'train'), (d_valid, 'valid')]

model2 = xgb.train(params, d_train, 200, watchlist, early_stopping_rounds=20)


# In[24]:

d_test = xgb.DMatrix(x_valid)
prediction2 = pd.DataFrame(model2.predict(d_test,ntree_limit=3))


# In[25]:

#Testing the Model.


# In[26]:

# Let us now test the model on the fresh data
#Load test data using the same variables as above.
my_path = '' #This path is Users input , obtained by clicking 'Browse' and selecting file from local machine.
my_file = '' # User input file


# In[27]:

#Function to load pdf files
def split(my_path,my_file):
    pdf = pyPdf.PdfFileReader(open(my_path+my_file, "rb"))
    all_text3 =''
    for page in pdf.pages:
        all_text3 = all_text3 + str(page.extractText().encode('utf-8'))

    return all_text3


# In[28]:

#Loading data in to a list
all_files = os.listdir(my_path)
approved_doc = []
all_files = all_files[1:]
for files in all_files:
    try:
        singledoc = split(my_path,files)
        approved_doc.append(singledoc)
    except Exception:
        pass


# In[29]:

#Put it into a dataframe
df_test = pd.DataFrame()
df_test['column1'] = ""
df_test['column2'] = ""


# In[30]:

#df_test.head(5)


# In[31]:
clean_rows = []
for rows in df_test.column1:
    rows = rows.lower()
    rows = re.sub('[^A-Za-z]+', ' ', rows)
    #rows = ' '.join([word for word in rows.split() if word not in stops])
    #rows=rows.strip()
    clean_rows.append(rows)


# In[32]:

'''test.fillna((-999), inplace=True)
for f in test.columns: 
    if test[f].dtype=='object': 
        lbl = preprocessing.LabelEncoder() 
        lbl.fit(list(test[f].values)) 
        test[f] = lbl.transform(list(test[f].values))'''


# In[33]:

#creating tfidf of the test set
train_test_tdidf_features_test = vectorizer.fit_transform(df_test.content)
train_test_tdidf_features_test = train_test_tdidf_features_test.toarray()
df_train_test_tfidf_features_test = pd.DataFrame(train_test_tdidf_features_test)
df_train_test_tfidf_features_test.columns = vectorizer.get_feature_names()


# In[34]:

#df_train_test_tfidf_features_test.head()


# In[35]:

#df_train_test_tfidf_features_test.shape


# In[36]:

#dataframe.index = pd.DataFrame(df_train_test_tfidf_features.transpose().index)


# In[37]:

train_tf_idf = pd.DataFrame()
train_tf_idf['feature'] = df_train_test_tfidf_features.columns
train_tf_idf.index = train_tf_idf.feature
#train_tf_idf.head(5)


# In[38]:

#train_tf_idf.shape


# In[39]:

test_tfidf = df_train_test_tfidf_features_test.transpose()
#test_tfidf


# In[40]:

test_tfidf['feature'] = test_tfidf.index
#test_tfidf.head()


# In[41]:

#print train_tf_idf.head(10)


# In[42]:

test_tfidf_final = pd.merge(train_tf_idf, test_tfidf, on='feature',how='left')
#print test_tfidf_final.head(5)


# In[43]:

test_tfidf_final.index = test_tfidf_final.feature


# In[44]:

del test_tfidf_final['feature']
#print test_tfidf_final


# In[45]:

#test_tfidf_final2 = test_tfidf_final.transpose()
test_tfidf_final_1 = test_tfidf_final.transpose()
#print test_tfidf_final_1.head(5)


# In[46]:

test_tfidf_final_1.fillna(0,inplace=True)
#print test_tfidf_final_1.head(5)


# In[47]:

#test_tfidf_final_1.head()


# In[48]:

#print predicted class
d_test = xgb.DMatrix(test_tfidf_final_1)
prediction2 = pd.DataFrame(model2.predict(d_test,ntree_limit=3))


# In[49]:

#print predicted probability
d_test = xgb.DMatrix(test_tfidf_final_1)
prediction = pd.DataFrame(model.predict(d_test,ntree_limit=3))


# In[50]:

#Displaying result of Prediction.
Prediction3 = pd.DataFrame()


# In[51]:

Prediction3 = prediction
#print Prediction3


# In[52]:

Final_Prediction = pd.concat([df_test['column1'],prediction2,prediction ], axis=1)
print Final_Prediction
#Final_Prediction.to_csv('Prediction_Test2.csv')

